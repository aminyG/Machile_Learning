{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical 2: Text Generator with RNN"
      ],
      "metadata": {
        "id": "Li4NWggjpjdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "Import tensorflow"
      ],
      "metadata": {
        "id": "nnvCrwnAp0PO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XlUeB7GKnMqA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset Shakespeare"
      ],
      "metadata": {
        "id": "1YwKksU0p9fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV8o0-4Zp41K",
        "outputId": "7431df2b-558b-43d7-9c8c-3a348f719e36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "4uCfyOG7qMmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RiNhmnxqFef",
        "outputId": "70b90da4-69c3-450a-a575-67698a5f06e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2OKbpkjqOlp",
        "outputId": "528e7a23-9c19-4b66-f7c4-2709a203f1d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZf5qRCqQJK",
        "outputId": "173ed58c-6e27-4c14-884b-fcee102a5f9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Processing"
      ],
      "metadata": {
        "id": "9uXifL7MqXNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize Text <br>\n",
        "Before training, you need to convert the string to a numeric representation. tf.keras.layers.StringLookup can convert each character to a numeric ID. The method is that the text will be broken down into tokens first."
      ],
      "metadata": {
        "id": "15d1efd8qZ-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5k8GGHtqR71",
        "outputId": "d318ec60-8bd9-4aa5-af15-a67c40092831"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now create a tf.keras.layers.StringLookup layer"
      ],
      "metadata": {
        "id": "ypilq7I-qk8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "yUZZNlZIqeZP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command above converts the token to an ID"
      ],
      "metadata": {
        "id": "BUETvIFqqug2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHl3j2Ggqn6O",
        "outputId": "892701b2-7fdd-476f-a031-d1bb6f4984c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal of this tutorial is to generate text, it is also important to invert this representation. For this you can use the code"
      ],
      "metadata": {
        "id": "gJPPMl_4rBCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "L9OuBJ3Eq7g1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer converts back the characters from the ID vector, and returns them as tf.RaggedTensor characters"
      ],
      "metadata": {
        "id": "F_Nm7Ir5rEZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IPfwZgfrC3M",
        "outputId": "b394e6a4-0448-4d73-f53d-108de1a1f2ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use tf.strings.reduce_join to rejoin characters into a string."
      ],
      "metadata": {
        "id": "AsT0DZWzrPMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADYfJaLzrMz6",
        "outputId": "9c1671f7-e8ae-4562-a983-21ace5b07898"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "NvGbKR11rQ0u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions\n",
        "Create Training Sets and Targets"
      ],
      "metadata": {
        "id": "vlaIJHDCrjqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fup7gFI5rYK4",
        "outputId": "710ff9f5-1967-4969-f7c2-8ff86370ad54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "AIULBkRprrI_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ3_hytUrxC9",
        "outputId": "a948817f-1bab-4bf6-a02e-73aa2980efbe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "4FYrexs9r2mN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch method allows you to easily convert these individual characters into a sequence of desired sizes."
      ],
      "metadata": {
        "id": "jN0VFJEer8YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9dQAa2Nr55C",
        "outputId": "f578c518-4f96-46e1-e0c8-fee6c5356013"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it will be easier to see what is done if you concatenate the tokens back into a string"
      ],
      "metadata": {
        "id": "SoowjXBssAz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5RtW_Zgr-PV",
        "outputId": "aa2ad113-eaa7-4c6a-daa0-0f766f851cee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training, you need a dataset of pairs (input, labels). Where input and label are sequences. At each time step, the input is the current character and the label is the next character. Here's a function that takes a sequence as input, duplicates it, and shifts it to align the input and labels for each time step"
      ],
      "metadata": {
        "id": "W31ZHsyTsHXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "FAPrHQuesFJe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osrsz9nmsMt6",
        "outputId": "abd950b4-17e0-428a-b5a6-bcf2f34731f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "wyErs8a2sPZM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs0nAo9SsVqQ",
        "outputId": "a1529e0f-d37b-4f6b-90d6-d1fe918f565c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Training Batches\n",
        "\n",
        "You use tf.data to divide text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "RvcpUzjRsZtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIHM_GnfsXpv",
        "outputId": "20d8c0e7-9940-4a53-e2bb-a59e31b6657b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Model"
      ],
      "metadata": {
        "id": "RYVTDxmasjFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "rij0TvZQsg4R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "TMVFDTYysmNI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "27bmNrCaviTb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Test"
      ],
      "metadata": {
        "id": "7ccFo116vmPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co_lshvLvkGi",
        "outputId": "43c02dfc-e3bd-41ce-ceb2-9acd70b82ed5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhkSFCc_vp1a",
        "outputId": "a0332dcf-5467-4311-99b4-c89b1b4bc5ee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the actual predictions from the model, you need to sample from the output distribution, to get the actual character index. This distribution is determined by the logit on the character vocabulary. Note: It is important to sample from this distribution because taking argmax from it can easily get the model stuck in an infinote loop. Try it for the first example in the batch"
      ],
      "metadata": {
        "id": "5J6Dw0Lov0dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "IltyhXh4vtER"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us, at each time step, a prediction of the next character index:"
      ],
      "metadata": {
        "id": "1rqsvYiJv9UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_UzUcH9v4Xd",
        "outputId": "9a7cfe40-448b-4485-807f-d952793524ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([53, 58, 17, 50,  0, 36,  0,  2, 57, 32,  3,  5,  0, 24, 49, 21, 11,\n",
              "       24, 17, 27, 12, 25, 37,  8, 61, 34, 27, 20, 29, 32, 46, 29, 39,  9,\n",
              "        3, 33, 10, 48, 56, 40, 53, 10, 50, 34, 11, 12, 23, 41, 43,  1, 49,\n",
              "       48,  7, 36, 48, 15, 63, 52, 26, 51, 55, 39, 56, 39, 44, 11, 40,  0,\n",
              "       12, 60, 30, 28, 24, 22, 49, 17, 17, 65, 38,  9, 63, 22, 54, 62, 51,\n",
              "       36, 31,  1, 21, 26, 56,  1,  6, 14, 65, 22, 30, 42, 27, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode the following code to see the text predicted by this untrained model:"
      ],
      "metadata": {
        "id": "b8ByTHnCv-mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOfJmKNFv6cQ",
        "outputId": "4a381e4f-b208-4f1a-93ef-376a18b776fd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"The Earl of Wiltshire hath the realm in farm.\\n\\nLORD WILLOUGHBY:\\nThe king's grown bankrupt, like a br\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"nsDk[UNK]W[UNK] rS!&[UNK]KjH:KDN;LX-vUNGPSgPZ.!T3iqan3kU:;Jbd\\nji,WiBxmMlpZqZe:a[UNK];uQOKIjDDzY.xIowlWR\\nHMq\\n'AzIQcNh\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Models\n",
        "Additional optimizer and loss function\n",
        "\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the final dimension of the prediction. Since your model returns logits, you need to set the from_logits flag."
      ],
      "metadata": {
        "id": "8nZ1r7w1wJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "yF960gXSwD5a"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn8OKRAKwOXG",
        "outputId": "12acbd1d-51ad-47a8-d942-9ae371ba5420"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1891813, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The newly initialized model should not be too confident in itself, all output logs should have the same magnitude. To confirm this, you can check that the exponential of the average loss should be approximately equal to the vocabulary size. A much higher loss means the model is confident in the wrong answer, and has bad initialization:"
      ],
      "metadata": {
        "id": "hxDxPnqhwYxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uVIL0ntwV0l",
        "outputId": "cbc374fc-1071-499a-f9a8-b8f96f126aad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.968765"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konfigurasikan prosedur pelatihan menggunakan metode tf.keras.Model.compile. Gunakan tf.keras.optimizers.Adam dengan argumen default dan fungsi loss."
      ],
      "metadata": {
        "id": "1rv2jlILwcAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "uEp03S-BwaRA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure Checkpoints\n",
        "\n",
        "Use tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ],
      "metadata": {
        "id": "3W2EqVBlwivn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "uHZ2G9BfwdcO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carry out the Training Process"
      ],
      "metadata": {
        "id": "ioc46UnBwsOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "sAsyP6REwqD7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeSK6R6OwwHf",
        "outputId": "d3fa8576-5f18-4f10-bb59-651d96d5177a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 20s 66ms/step - loss: 2.7050\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.9756\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.7011\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.5441\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.4479\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3810\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3284\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.2844\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.2425\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.2031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Text"
      ],
      "metadata": {
        "id": "fEHQfA0P21x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "JOob492Fw0m3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "_y2qzSFb4XbG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4PkNh8K4-fl",
        "outputId": "60c9b931-e1ed-4cd7-d991-2a32ba3eb6ba"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Not rooting to the bloody, he just more fire.\n",
            "\n",
            "DUKE OF YORK:\n",
            "She, who holds me sour me from that word with her?\n",
            "My son Pargaret, which divinest a widow,\n",
            "Exceeding that hath nothing of good stopp'd with the\n",
            "discourteous, live you, worsensio, for he should have more beart:\n",
            "My daughter, and Menenius, whose hands and me\n",
            "Have twenting to be consul: but one this bringdal trud\n",
            "Made jest his wife's night.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Your manny, my brother?\n",
            "\n",
            "COMINIUS:\n",
            "But, let him slain,\n",
            "Where are your ladys and blood with a gracital buttobles\n",
            "As it as my sister unake!\n",
            "Go take them again: I shall well exame to such is fool:\n",
            "Unse of goodsming smoths so tidow 'gainst thy profess\n",
            "On your hearts do and with his son-socreach. Fie, I would no.\n",
            "\n",
            "GLOUCESTER:\n",
            "I cannot jeaud waste of one.\n",
            "\n",
            "Provost:\n",
            "Swift, let him?\n",
            "\n",
            "PETRUCHIO:\n",
            "What, nor none, that, I know not. My sovereign;\n",
            "Metting the Catis and honour Sich\n",
            "Shasing our actions and wims within the liss,\n",
            "Like templessed with a dispatch, or life\n",
            "Some league and life i \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5854804515838623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLClkmmg5kGP",
        "outputId": "3334dc7d-c19c-4930-8a4b-2b97a653d6ca"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nHow now!\\nThe Duke of Bolingbroke, for if you set my son,\\nI will true that shall be arms'd with a togh\\nOf gracious for a line, lady,\\nRight two God will obe thee, Ere your last\\nUnder your coward, were indubed, trust.\\n\\nPROSPERO:\\nBroke, thou art dety?\\nAnd he is noble grumious mother?'\\n\\nFirst Citizen:\\nHow now, my lord; fear that, in goie, horse; confride thee.\\n\\nPETRUCHIO:\\nWhy, then, drange me, my lord: scape you, pray!\\n\\nFirst Lady:\\nIf you, Joss' is for a fetter\\nHad indeed, but indeed, but for what's the master;\\nFor Allibe in our mother, she is in the least,\\nThat I am a gine as well as you!\\n\\nCAMILLO:\\nGo to the warleys King Histings, one father,\\nA kinder matter, he is before thyself;\\nFor chreits I heard thou warr'd the first.\\n\\nThird Servingman:\\nDo it well.\\n\\nANGELO:\\nThere? if you may well! if you nothing.\\n\\nCAMILLO:\\nBy sweeting, this a gloot,\\nYet I comal of his. Do him my sorrow: if all she doth mother approach:\\nI willing at the place, O examp to Rith Clifford! how I pursuay,\\nAnd accirent agai\"\n",
            " b\"ROMEO:\\nThen? and, for a soul, do not speak; from thy dishonour lie\\nShe mayor thou shalt live.\\n\\nVOLUMNIA:\\nI know' the ward of this:\\nLook what's he requitest those that we hear.\\nBut what shell your lord?\\n\\nKATHARINA:\\nA vaster throckness not befell?\\nBut that your ship, why should yourself is not\\nThis too hell, let thy thoughts: I have begin in subject\\nI: in the helding of this land,\\nIf this we lie: I speak a'll well you note:\\nThat, I know not to make me not.\\n\\nPETRUCHIO:\\nFaith, master, good bestress: sid.\\n\\nLOPSS:\\nPethup, may speak all mercition!\\nCousin! our reers must be bornetime of myself?\\nOr else you seal his good eyes; his charity,\\nCuest too off, the scolening of my gragefullus'\\nOn part enough and terves of weeks: in whose none,\\nClaudio's pities, that smowed, indreditoble summed disgrace'd;\\nNo fortune his lorders say I wave her.\\n\\nPAULINA:\\nHow now, my life, I'll prove. Thou mayst-flay'd glost!\\nPlease our king.\\n\\nDUKE OF AUMERLE:\\nWhy, Buly shadow, I cannot sway the holy pardon.\\n\\nDUKE OF AUMERLE:\"\n",
            " b\"ROMEO:\\nBelieve me, she's a traitor lord; I say.\\n\\nBRUTUS:\\nNow, if thou offers? Why read me better lip on, go but so;\\nWe have I had as if yow make ut all.\\nTell me, Is you wend to their gird.\\n\\nJOHN OF YORK:\\nWhipp thou sworn tender than my care?\\n\\nKING EDWARD IV:\\nBut smacred that, I swear' the groon for parliament\\nIn am a shinening gland more witness\\nDicks his gunglent to be ajupon. Kame thy root up,\\nTurn in the devil's mother old man.\\nI never had my followers: I come.\\n\\nABRAANLA:\\nNot that: my brother: I hope it:\\nThine issue for our offirely son:\\nSee how the statutes that go dine?'\\nYou throw no mightory! Pardon me,\\nBut then I saw them to the fawnard hell.\\n\\nKING RICHARD II:\\nLet me make me life.\\nAh further, how now, Marcius,\\nYou wry, whiles I in these that should call'd me or\\nyou: profaned, in play, whose lords again\\nTo your highness or goodly man, thou had, and ours\\nLandst her curse she stand: but I should not be yound,\\nThan those of importuning married in the world,\\nLook, whate's without my daught\"\n",
            " b\"ROMEO:\\nOf you, sir? hark! what, dovil, indretch man we fain than it.\\nThou, Savourior,--ancions, sir; I will disdain,\\nmy mother, love-have been award me up the lady:\\nOrner, my hand, condolen, speak; for, being on\\nattempt, and he's worse; and love-beast,\\nA move, sir. Since his admiral,\\nGloucester'd birds of God.'\\n\\nDUCHESS:\\nYour love I scall askempty, sometimes Kighal,\\nSayn'd you have wither, vow, that it strikes and make those\\nThat stroughts must bear it before him goody.\\n\\nVOLUMNIA:\\nThen, I'll hang upon, like an hundred noble.\\nSoe for him! Prosper?\\n\\nBUCKINGHAM:\\nNay, that away arms:\\nI cannot more than else, thou art not flowe;\\nFor will I die. We come to me:\\nThe yeasth of the own mourn groans, boaths,\\nAs shooped that the put men\\nIn spirits of, with one than earth in Mannaw:\\nA little fight, go; help, hold, his good lord, girt inds as a pity;\\nTears that will to be known'd by the common: perform my bass\\nAs with a frighter clod-country.\\n\\nISABELLA:\\n'Tis too: what crown our drums.\\nHail, mistress; they\"\n",
            " b\"ROMEO:\\nGood king!' quescones, what will I have done.\\nNot neither speed. Look, noble; but hath new,\\nWith kings Kill from fortufe to be amorous.\\nVery welcome loss. Onr'sting; Apare,' many him with country?\\n\\nCATESBY:\\nI speak forbed with a blame coming some news.\\nO lame! loved the dost, I have it goes.\\n\\nKING RICHARD III:\\nSpeak stopt as most\\nhonour'd that a misconnicted: yit wear your honours to do beat\\nMade bodeship that says me an arm'd eye,\\nSee, she coward: let him construme their words unjest;\\nThou callents not to: for my father's fury;\\nPour shedeful helm at nobling frowls shall be obstance.\\n\\nDUKE VINCl:\\nSole new consul? The wiseful command carry our hands:\\nHe sinst I know him therein only gooders.\\n\\nYORK:\\nNow, for about with aum, I have.\\n\\nPETRUCHIO:\\nAre you so go,\\nLoat seemed to me to keep your honour.\\n\\nNOMTHAM:\\nWhat is forced by this way to shame.\\n\\nKING RICHARD III:\\nWhy she well deserved him gladly glory than it.\\n\\nFirst Mustread:\\nYou have often thanks, sir, if it boy.\\n\\nWARWICK:\\nHear you well\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.002254009246826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export Generator Model"
      ],
      "metadata": {
        "id": "McUeb5sI5sTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rocV5zAL5ok2",
        "outputId": "45af914d-4801-4371-ff4a-fa834b76d45f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7ee89012a0e0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD8u-r395wBg",
        "outputId": "d4cd2e3a-105f-449e-e48b-fdb307fb5498"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Ay, what I cannot proceed the\n",
            "offences? why, most warrio!\n",
            "Is it softended-mouth, Treachedine I'ld\n",
            "w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS"
      ],
      "metadata": {
        "id": "P2yDANSZ55oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide.\n",
        "\n",
        "Prosedurnya adalah :\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ],
      "metadata": {
        "id": "br-tff9N5_yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "avclvjfY51E2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "v7-x7qUE6ITo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "M0c-LOzw6NJE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEM2O7Ap6Rvk",
        "outputId": "d8f4f436-3e04-4778-e559-67ed43d5697c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 63ms/step - loss: 2.7239\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ee8900dfac0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnOzeVKD6VV_",
        "outputId": "304433b6-5ec2-4d47-861e-a639396e7ed8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1800\n",
            "Epoch 1 Batch 50 Loss 2.0702\n",
            "Epoch 1 Batch 100 Loss 1.9699\n",
            "Epoch 1 Batch 150 Loss 1.8791\n",
            "\n",
            "Epoch 1 Loss: 1.9944\n",
            "Time taken for 1 epoch 16.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8015\n",
            "Epoch 2 Batch 50 Loss 1.7387\n",
            "Epoch 2 Batch 100 Loss 1.6774\n",
            "Epoch 2 Batch 150 Loss 1.6475\n",
            "\n",
            "Epoch 2 Loss: 1.7140\n",
            "Time taken for 1 epoch 13.46 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5875\n",
            "Epoch 3 Batch 50 Loss 1.5583\n",
            "Epoch 3 Batch 100 Loss 1.5670\n",
            "Epoch 3 Batch 150 Loss 1.5454\n",
            "\n",
            "Epoch 3 Loss: 1.5538\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4734\n",
            "Epoch 4 Batch 50 Loss 1.4529\n",
            "Epoch 4 Batch 100 Loss 1.4420\n",
            "Epoch 4 Batch 150 Loss 1.4389\n",
            "\n",
            "Epoch 4 Loss: 1.4535\n",
            "Time taken for 1 epoch 11.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3856\n",
            "Epoch 5 Batch 50 Loss 1.4070\n",
            "Epoch 5 Batch 100 Loss 1.4073\n",
            "Epoch 5 Batch 150 Loss 1.3438\n",
            "\n",
            "Epoch 5 Loss: 1.3863\n",
            "Time taken for 1 epoch 14.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3612\n",
            "Epoch 6 Batch 50 Loss 1.3226\n",
            "Epoch 6 Batch 100 Loss 1.3498\n",
            "Epoch 6 Batch 150 Loss 1.3247\n",
            "\n",
            "Epoch 6 Loss: 1.3336\n",
            "Time taken for 1 epoch 12.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.3013\n",
            "Epoch 7 Batch 50 Loss 1.3078\n",
            "Epoch 7 Batch 100 Loss 1.2602\n",
            "Epoch 7 Batch 150 Loss 1.3133\n",
            "\n",
            "Epoch 7 Loss: 1.2890\n",
            "Time taken for 1 epoch 13.64 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2089\n",
            "Epoch 8 Batch 50 Loss 1.2357\n",
            "Epoch 8 Batch 100 Loss 1.2768\n",
            "Epoch 8 Batch 150 Loss 1.2733\n",
            "\n",
            "Epoch 8 Loss: 1.2473\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1747\n",
            "Epoch 9 Batch 50 Loss 1.2219\n",
            "Epoch 9 Batch 100 Loss 1.2077\n",
            "Epoch 9 Batch 150 Loss 1.2408\n",
            "\n",
            "Epoch 9 Loss: 1.2077\n",
            "Time taken for 1 epoch 11.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1829\n",
            "Epoch 10 Batch 50 Loss 1.1430\n",
            "Epoch 10 Batch 100 Loss 1.1915\n",
            "Epoch 10 Batch 150 Loss 1.1677\n",
            "\n",
            "Epoch 10 Loss: 1.1679\n",
            "Time taken for 1 epoch 12.33 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Assignment"
      ],
      "metadata": {
        "id": "oVOERBiB9Yz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the code above and state the difference with practicum 2?\n",
        "\n",
        "- The difference between assignment code and practicum 2 lies in the training approach used. In practicum 2, a simple training approach using **'model.fit'** was used. However, in task codes, a more specialized and complex training approach is implemented. In this particular approach, a **'train_step'** method is defined in the model instance, which controls training at the batch level. This method explicitly calculates loss values, gradients, and updates model weights. Additionally, the **'tf.metrics.Mean'** object is used to calculate the average loss during training. So it can be concluded that this approach provides a lot of control and high flexibility in model training"
      ],
      "metadata": {
        "id": "5gQ1P0Ke7BmP"
      }
    }
  ]
}